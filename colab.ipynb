{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install diffusers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from train import train, save\n",
    "from model import VaReSynth\n",
    "from demo import demo\n",
    "\n",
    "# Create the model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = VaReSynth().to(device)\n",
    "# model_ema = deepcopy(model)\n",
    "print('Model parameters:', sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=2e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Use a low discrepancy quasi-random sequence to sample uniformly distributed\n",
    "# timesteps. This considerably reduces the between-batch variance of the loss.\n",
    "rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
    "\n",
    "epoch = 0\n",
    "while True:\n",
    "    train(model, opt, scaler, rng, epoch)\n",
    "    save(model, opt, scaler, epoch)\n",
    "    epoch += 1\n",
    "    if epoch % 5 == 0:\n",
    "        demo(model, epoch)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
